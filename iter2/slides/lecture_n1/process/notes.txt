The new edition has been retitled Neural Networks and Learning Machines, in order to
reflect two realities:
1. The perceptron, the multilayer perceptron, self-organizing maps, and neurodynamics,
to name a few topics, have always been considered integral parts of
neural networks, rooted in ideas inspired by the human brain.
2. Kernel methods, exemplified by support-vector machines and kernel principalcomponents
analysis, are rooted in statistical learning theory.



The brain is
a highly complex, nonlinear, and parallel computer (information-processing system). It
has the capability to organize its structural constituents, known as neurons, so as to
perform certain computations (e.g., pattern recognition, perception, and motor control)
many times faster than the fastest digital computer in existence today



A neural network is a massively parallel distributed processor made up of simple processing
units that has a natural propensity for storing experiential knowledge and making it available
for use. It resembles the brain in two respects:
1. Knowledge is acquired by the network from its environment through a learning process.
2. Interneuron connection strengths, known as synaptic weights, are used to store the acquired
knowledge.



Benefits of Neural Networks
	Nonlinearity
	Input–Output Mapping
	Adaptivity
	Evidential Response
	Contextual Information 
		Knowledge is represented by the very structure and
		activation state of a neural network. Every neuron in the network is potentially affected
		by the global activity of all other neurons in the network. Consequently, contextual
		information is dealt with naturally by a neural network.
	Fault Tolerance
	VLSI Implementability
		The massively parallel nature of a neural network makes
		it potentially fast for the computation of certain tasks.This same feature makes a neural
		network well suited for implementation using very-large-scale-integrated (VLSI) technology.
		One particular beneficial virtue of VLSI is that it provides a means of capturing
		truly complex behavior in a highly hierarchical fashion (Mead, 1989).
	Uniformity of Analysis and Design
		Basically, neural networks enjoy universality
		as information processors.We say this in the sense that the same notation is used in
		all domains involving the application of neural networks.
	Neurobiological Analogy



Typically, neurons are five to six orders of magnitude
slower than silicon logic gates; events in a silicon chip happen in the nanosecond range,
whereas neural events happen in the millisecond range. However, the brain makes up
for the relatively slow rate of operation of a neuron by having a truly staggering number
of neurons (nerve cells) with massive interconnections between them. It is estimated
that there are approximately 10 billion neurons in the human cortex, and 60 trillion
synapses or connections (Shepherd and Koch, 1990). The net result is that the brain is
an enormously efficient structure. Specifically, the energetic efficiency of the brain is approximately
10-16 joules (J) per operation per second, whereas the corresponding value
for the best computers is orders of magnitude larger.



Математическая модель искусственного нейрона была предложена У. Маккалоком и У. Питтсом вместе с моделью сети, состоящей из этих нейронов. Авторы показали, что сеть на таких элементах может выполнять числовые и логические операции[4]. Практически сеть была реализована Фрэнком Розенблаттом в 1958 году как компьютерная программа, а впоследствии как электронное устройство — перцептрон.



However, we may solve the XOR problem by using a single hidden layer with two
neurons, as in Fig. 4.8a (Touretzky and Pomerleau, 1989).
142 page











Geoff Hinton, who is a famous professor of ML at the University of Toronto, has said:
When we’re learning to see, nobody’s telling us what the right answers are — we just
look. Every so often, your mother says “that’s a dog”, but that’s very little information.
You’d be lucky if you got a few bits of information — even one bit per second — that
way. The brain’s visual system has 1014 neural connections. And you only live for 109
seconds. So it’s no use learning one bit per second. You need more like 105 bits per
second. And there’s only one place you can get that much information: from the input
itself. — Geoffrey Hinton, 1996 (quoted in (Gorder 2006)).











