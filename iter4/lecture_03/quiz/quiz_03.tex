\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[OT1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm]{geometry}

\author{Nikolay Anokhin}

\begin{document}

\thispagestyle{empty}

\subsection*{задание 1 (0.25)}

Принцип максимального правдоподобия (maximum likelihood): \\
(а) отдает предпочтение параметрам модели, которые дают максимальную вероятность наблюдаемых данных из data set \\
(б) требует предположений о форме вероятностных распределений, генерирующих данные \\
(в) подразумевает только численную оптимизацию функции правдоподобия \\
(г) подразумевает только аналитическую оптимизацию функции правдоподобия \\
(д) не применим для распределений, отличных от нормальных

\subsection*{задание 2 (0.25)}

Пусть дана нечестная монетка с неизвестным распределением вероятности (то есть мы не знаем, какая сторона перевешивает и на сколько). После серии испытаний функция правдоподобия для полученных данных имеет форму \\
(а) плотности вероятности распределения Бернулли \\
(б) плотности вероятности биномиального распределения \\
(в) плотности вероятности распределения Пуассона \\
(г) плотности вероятности равномерного распределения

\subsection*{задание 3 (0.25)}

Алгоритм Expectation Maximization \\
(а) позволяет численно находить maximum-likelihood решения для сложных распределений \\
(б) гарантирует нахождение глобального максимума функции правдоподобия \\
(в) всегда сходится \\
(г) имеет итеративную природу, причем на каждой итерации функция правдоподобия неубывает

\subsection*{задание 4 (0.25)}

Алгоритм k-means++ -- это \\
(а) обычный k-means, но обеспечивающий глобальную сходимость \\
(б) обычный k-means, но с улучшенной инициализацией \\
(в) быстрая реализация k-means \\
(г) плод воображения Николая Анохина. Такого алгоритма не существует \\

\subsection*{задание 1 (0.25)}

Принцип максимального правдоподобия (maximum likelihood): \\
(а) отдает предпочтение параметрам модели, которые дают максимальную вероятность наблюдаемых данных из data set \\
(б) требует предположений о форме вероятностных распределений, генерирующих данные \\
(в) подразумевает только численную оптимизацию функции правдоподобия \\
(г) подразумевает только аналитическую оптимизацию функции правдоподобия \\
(д) не применим для распределений, отличных от нормальных

\subsection*{задание 2 (0.25)}

Пусть дана нечестная монетка с неизвестным распределением вероятности (то есть мы не знаем, какая сторона перевешивает и на сколько). После серии испытаний функция правдоподобия для полученных данных имеет форму \\
(а) плотности вероятности распределения Бернулли \\
(б) плотности вероятности биномиального распределения \\
(в) плотности вероятности распределения Пуассона \\
(г) плотности вероятности равномерного распределения

\subsection*{задание 3 (0.25)}

Алгоритм Expectation Maximization \\
(а) позволяет численно находить maximum-likelihood решения для сложных распределений \\
(б) гарантирует нахождение глобального максимума функции правдоподобия \\
(в) всегда сходится \\
(г) имеет итеративную природу, причем на каждой итерации функция правдоподобия неубывает

\subsection*{задание 4 (0.25)}

Алгоритм k-means++ -- это \\
(а) обычный k-means, но обеспечивающий глобальную сходимость \\
(б) обычный k-means, но с улучшенной инициализацией \\
(в) быстрая реализация k-means \\
(г) плод воображения Николая Анохина. Такого алгоритма не существует \\

\end{document}