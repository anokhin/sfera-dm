\documentclass[aspectratio=169]{beamer}
%\usetheme{Marburg}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[OT1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{empheq}
\usepackage[many]{tcolorbox}
\usepackage{multirow}

\author{Николай Анохин}

\title{Краткое введение в data mining}
%\setbeamercovered{transparent} 
\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
\date{} 
%\subject{}

\tcbset{highlight math style={enhanced,colframe=red,colback=white,arc=4pt,boxrule=1pt}}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Задача data mining}

\begin{frame}{Data Mining как KDD}

\begin{quote}{Knowledge Discovery in Databases (KDD)}
-- это процесс получения точных, неизвестных, потенциально полезных и интерпретируемых закономерностей из данных.\footnote{U. Fayyad, G. Piatetsky-Shapiro, P. Smyth. From data mining to knowledge discovery: an overview. 1996}
\end{quote}

\end{frame}

\begin{frame}{Data Mining как моделирование}

\begin{quote}{Data Mining}
-- процесс построения модели, хорошо описывающей закономерности, которые порождают данные.
\end{quote}

Подходы к построению моделей
\begin{itemize}
\item cтатистический
\item машинное обучение
\item вычислительный
\end{itemize}

\end{frame}

\begin{frame}
\frametitle<1>{Качество вина\footnote{\href{https://archive.ics.uci.edu/ml/datasets/Wine+Quality}{Wine Quality Data Set. UCI Machine Learning Repository}}}
\frametitle<2>{Качество вина: статистический подход}
\frametitle<3>{Качество вина: машинное обучение}
\frametitle<4>{Качество вина: вычислительный подход}

\begin{columns}[c]
    \begin{column}{.49\linewidth}
    \begin{footnotesize}
    \begin{overprint}
				\onslide<1>
				\begin{center}			
			\begin{tabular}{r  c  c}
			& ABV, \% & Quality \\
			\hline
			1 & 12.8 & good \\
			2 & 12.8 & good \\
			3 & 10.5 & good \\
			4 & 10.7 & good \\
			5 & 10.7 & good \\
			$\ldots$ & $\ldots$ & $\ldots$ \\
			198 & 11.4 & good \\
			199 & 10.10 & bad \\
			200 & 10.30 & bad \\
			201 & 10.90 & bad \\
			202 & 9.95 & bad \\
			$\ldots$ & $\ldots$ & $\ldots$ \\
			444 & 9.05 & bad \\
			\end{tabular}
			\end{center}
			  \onslide<2>
			  \[
			\begin{cases}
			p( \text{alcohol} \mid \text{good} ) \sim \mathcal{N}(\text{alcohol} \mid \mu_g, \sigma_g) \\
			p( \text{alcohol} \mid \text{bad} ) \sim \mathcal{N}(\text{alcohol} \mid \mu_b, \sigma_b) \\
			\end{cases}
			\]
			\[
			\qquad\qquad\Downarrow (\text{ML-принцип})
			\]
			\[
			\begin{cases} 
			\mu_g=11.4, \sigma_g=1.3 \\ 
			\mu_b=10.2, \sigma_b=1.0
			\end{cases}
			\]
			  \onslide<3> 
			  Обучаем линейный SVM: 
			  \[\text{alcohol} > 11.2 \Rightarrow \text{good}\]
			  \onslide<4>
			  Подсчитываем параметры данных: \[\langle \text{alcohol} \rangle_g = 11.4,\; \langle \text{alcohol} \rangle_b = 10.2\]		
		\end{overprint}
		\end{footnotesize}
    \end{column}    
    \begin{column}{.49\linewidth}
    \begin{center}
\includegraphics[width=0.95\textwidth]{images/wine.png}
\end{center}   
    \end{column}
\end{columns}

\end{frame}

\begin{frame}{Data Mining -- область на пересечении дисциплин\footnote{\href{http://blogs.sas.com/content/subconsciousmusings/2014/08/22/looking-backwards-looking-forwards-sas-data-mining-and-machine-learning/}{Looking backwards, looking forwards: SAS, data mining, and machine learning}}}

\begin{center}
\includegraphics[width=0.75\textwidth]{images/data-mining-venn.png}
\end{center}

\end{frame}

\begin{frame}{Data Mining -- область тысячи имен}

\begin{enumerate}
\item[1960-е] Data Fishing, Data Dredging
\item[1980-е] Knowledge Discovery in Databases
\item[1990-е] Data Mining, Database mining\textsuperscript{TM}
\item[2000-е] Data Analytics, Data Science\footnote{\href{https://twitter.com/nivertech/status/180109930139893761}{Data Scientist is a Data Analyst who lives in California}}\footnote{\href{https://twitter.com/josh_wills/status/198093512149958656}{A data scientist is someone who is better at statistics than any software engineer and better at software engineering than any statistician.}}
\end{enumerate}

\end{frame}

\begin{frame}{Некоторые важные события в истории Data Mining}

\begin{enumerate}
\item[1989] IJCAI-89 Workshop on Knowledge Discovery in Databases 
\item[1995] ACM SIGKDD Conference on Knowledge Discovery and Data Mining
\item[2001] Leo Breiman's ``Statistical Modeling: The Two Cultures''
\item[2003] Программа Total Information Awareness
\item[2005] Doug Cutting и Mike Cafarella разработали пакет обработки данных Hadoop
\item[2007] Первый релиз библиотки scikit-learn
\item[2010] Заработал сайт Kaggle -- платформа для проведения соревнований по Data Science
\item[2012] Harvard Business Review публикует статью Data Scientist: The Sexiest Job of the 21st Century
\item[2013] Первая встреча сообщества Moscow Data Science\footnote{\url{http://www.meetup.com/Moscow-Data-Science/}} в московском офисе Mail.Ru Group
\end{enumerate}

\end{frame}

\begin{frame}{CRISP-DM}

(Cross Industry Standard Process for Data Mining)

\begin{center}
\includegraphics[height=0.7\textheight]{images/crisp.png}
\end{center}

\end{frame}

\begin{frame}{Игра в гольф\footnote{Induction of Decision Trees / R. Quinlan}}

\begin{columns}
    \begin{column}{.7\linewidth}
    \begin{small}
    	{\bf Business understanding}
		\begin{itemize}
		\item понимание задачи с точки зрения бизнеса
		\item сбор требований и ограничений
		\item постановка задачи в терминах Data Mining
		\end{itemize}
		
		$\mathcal{D}$ -- множество, содержащее все рассматриваемые в задаче объекты \\  \vspace{1em}
		$f: \mathcal{D} \rightarrow \mathcal{Y}$ -- целевая функция \\ \vspace{1em}
		
		Цель -- с использованием данных о конечном множестве объектов из $\mathcal{D}$ (data set) научиться предсказывать значения целевой функции для любых объектов из $\mathcal{D}$
		
		\vspace{1em}
		Задача {\bf с учителем} -- для ``известных'' объектов дано значение целевой функции, иначе -- задача {\bf без учителя}.	
				\end{small}
		
    \end{column}
    %   
    \begin{column}{.3\linewidth}
    \vspace{-0em}
		\begin{center}
   		\includegraphics[width=\textwidth]{images/crisp.png}
    \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Игра в гольф}

\begin{columns}
    \begin{column}{.7\textwidth}
		{\bf Data understanding}
		\begin{itemize}
		\item первичный сбор данных
		\item ознакомление с данными и понимание их специфики
		\end{itemize}
		
		\vspace{1em}

		{\bf Data preparation}
		\begin{itemize}
		\item формирование финального набора данных
		\end{itemize}
    \end{column}
    %   
    \begin{column}{.3\textwidth}
    \vspace{-0em}
		\begin{center}
   		\includegraphics[width=\textwidth]{images/crisp.png}
    \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Признаки}

$\mathcal{D}$ -- множество, содержащее все рассматриваемые в задаче объекты \\
$d \in \mathcal{D}$ --  объект,  $\phi_j: \mathcal{D} \rightarrow F_j$ -- признак
    
\vspace{1em}
Виды признаков
\begin{itemize}
\item Бинарные/Binary \\ $F_j = \{true, false\}$
\item Номинальные/Categorical \\ $F_j$ -- конечное
\item Порядковые/Ordinal \\ $F_j$ -- конечное, упорядоченное
\item Количественные/Numerical \\ $F_j = \mathbb{R}$
\end{itemize}
\vspace{1em}
Признаковое представление объекта $d$
\[
\mathbf{x} = (\phi_1(d), \ldots, \phi_m(d)) \in \mathcal{X}
\]


\end{frame}

\begin{frame}{Игра в гольф: признаки}

\begin{center}
\begin{tabular}{c | c | c | c | c }
\bf Outlook & \bf Temperature & \bf Humidity & \bf Wind & \bf Play \\
\hline
Sunny & 85 & 85 & false & no \\
Sunny & 80 & 90 & true & no \\
Overcast & 83 & 86 & false & yes \\
Rainy & 70 & 96 & false & yes \\
Rainy & 68 & 80 & false & yes \\
Rainy & 65 & 70 & true & no \\
Overcast & 64 & 65 & true & yes \\
Sunny & 72 & 95 & false & no \\
Sunny & 69 & 70 & false & yes \\
Rainy & 75 & 80 & false & yes \\
Sunny & 75 & 70 & true & yes \\
Overcast & 72 & 90 & true & yes \\
Overcast & 81 & 75 & false & yes \\
Rainy & 71 & 91 & true & no \\
\end{tabular}
\end{center}

\end{frame}

\begin{frame}{Моделирование}

\begin{columns}
\begin{column}{.7\textwidth}
    \begin{itemize}
\item перебор различных моделей
\item настройка параметров моделей
\end{itemize}

\begin{block}{Модель}
признаковое описание объекта $d$:
\[
\mathbf{x} = (\phi_1(d), \ldots, \phi_m(d)) \in \mathcal{X}
\]
значение целевой функции для объекта $d$: $f(d) = y \in \mathcal{Y}$

\vspace{1em}
{\bf модель} -- семейство функций вида
\begin{empheq}[box=\tcbhighmath]{align}
H = \{h(\mathbf{x}, \theta) \,:\, \mathcal{X} \times \Theta \rightarrow \mathcal{Y} \},\nonumber
\end{empheq}
где $\theta \in \Theta$ -- неизвестный вектор параметров

\end{block}
    \end{column}
    %   
    \begin{column}{.3\textwidth}
    \vspace{-0em}
		\begin{center}
   		\includegraphics[width=\textwidth]{images/crisp.png}
    \end{center}
    \end{column}
  \end{columns}
    
\end{frame}

\begin{frame}{Качество вина}
    
{\bf признаковое описание}: $\mathbf{x} \in \mathbb{R}^1$ \\
\vspace{1em}
{\bf целевая переменная}: $y = 1$, если вино хорошее, $y = 0$ иначе \\
\vspace{1em}
{\bf модель}:
\[
\begin{cases}
p(\mathbf{x} | \text{good}) \sim \mathcal{N}(\mathbf{x} | \mu_g, \sigma_g), \;\; p(\text{good}) = \frac 1 2 \\
p(\mathbf{x} |  \text{bad}) \sim \mathcal{N}(\mathbf{x} | \mu_b, \sigma_b), \;\; p( \text{bad}) = \frac 1 2
\end{cases} \;\; + \quad y = \mathcal{I}(p(\text{good} | \mathbf{x}) > p(\text{bad} | \mathbf{x}))
\]
\vspace{1em}
{\bf параметры}: $\theta = (\mu_g, \sigma_g, \mu_b, \sigma_b)$

\end{frame}

\begin{frame}{Дерево решений}
    
\begin{columns}
\begin{column}{.5\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{images/dtree.png}
\end{center}
\end{column}
%   
\begin{column}{.5\textwidth}
\begin{tiny}
\begin{center}
\begin{tabular}{c | c | c | c | c }
\bf Outlook & \bf Temperature & \bf Humidity & \bf Wind & \bf Play \\
\hline
Sunny & 85 & 85 & false & no \\
Sunny & 80 & 90 & true & no \\
Overcast & 83 & 86 & false & yes \\
Rainy & 70 & 96 & false & yes \\
Rainy & 68 & 80 & false & yes \\
Rainy & 65 & 70 & true & no \\
Overcast & 64 & 65 & true & yes \\
Sunny & 72 & 95 & false & no \\
Sunny & 69 & 70 & false & yes \\
Rainy & 75 & 80 & false & yes \\
Sunny & 75 & 70 & true & yes \\
Overcast & 72 & 90 & true & yes \\
Overcast & 81 & 75 & false & yes \\
Rainy & 71 & 91 & true & no \\
\end{tabular}
\end{center}
\end{tiny}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Обучение модели}

\begin{itemize}
\item дана обучающая выборка (data set) $X = \{{\mathbf{x}_1, \ldots, \mathbf{x_N}}\}$
\item для каждого из объектов обучающей выборки дано значение целевой функции $Y = \{y_1, \ldots, y_N\}$ (если задача с учителем)
\end{itemize}

\begin{block}{Алгоритм обучения}
Выбор наилучших параметров $\theta^*$ с использованием обучающей выборки
\[
A(X, Y): (\mathcal{X} \times \mathcal{Y})^N \rightarrow \Theta
\]
В итоге:
\[
h^*(\mathbf{x}) = h(\mathbf{x}, \theta^*)
\]
\end{block}

\end{frame}

\begin{frame}{Пример 2. Игра в гольф}

\begin{columns}
    \begin{column}{.7\textwidth}
		{\bf Evaluation}
		\begin{itemize}
		\item тщательная проверка качества модели
		\item подробное рассмотрение шагов, предпринятых при построении
		\item поиск бизнес-требований, которые не удоволетворены
		\end{itemize}
		
		\vspace{1em}

		{\bf Deployment}
		\begin{itemize}
		\item презентация модели клиенту
		\item развертывание и использование модели
		\end{itemize}
    \end{column}
    %   
    \begin{column}{.3\textwidth}
    \vspace{-0em}
		\begin{center}
   		\includegraphics[width=\textwidth]{images/crisp.png}
    \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Другие процессы: SEMMA\footnote{http://timkienthuc.blogspot.ru/2012/04/crm-and-data-mining-day-08.html}, KDD\footnote{http://www.rithme.eu/}}

\begin{columns}
    \begin{column}{.5\textwidth}
    	\includegraphics[width=\textwidth]{images/semma.png}
    \end{column}
       
    \begin{column}{.5\textwidth}
    \vspace{-0em}
	\begin{center}
   		\includegraphics[width=\textwidth]{images/kdd.png}
    \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Задача кластеризации}

В задачах кластеризации целевая переменная не задана. Цель -- отыскать ``скрытую структуру'' данных.

\vspace{1em}
{\bf Дано.} Признаковые описания $N$ объектов $\mathbf{x} \in \mathcal{X}$, образующие тренировочный набор данных $X$

\vspace{1em}
{\bf Найти.} Модель из семейства параметрических функций 
\[
H = \{h(\mathbf{x, \mathbf{\theta}}): \mathcal{X} \times \Theta \rightarrow \mathcal{Y} \mid \mathcal{Y} = \{1, \ldots, K\}\},
\]
ставящую в соответствие произвольному $\mathbf{x} \in \mathcal{X}$ один из $K$ кластеров так, чтобы объекты внутри одного кластера были похожи, а объекты из разных кластеров различались

\end{frame}


\end{document}