{
 "metadata": {
  "name": "",
  "signature": "sha256:c2d85f01a887ed3ff785907ecdc07f3ffce6b192d082b8b48d5760b08e0544bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435 4. \u041a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438\u0437 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u0421\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f - \u0441\u0431\u043e\u0440 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0442\u044c\u0441\u044f \u043a Twitter API \u0438 \u0437\u0430\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0442\u044c \u0442\u0432\u0438\u0442\u044b \u043f\u043e id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f. \n",
      "\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a API \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u043e \u0432 \u0414\u0417 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "\n",
      "CONSUMER_KEY = \"NIdJmEzKanvYMoZMQOLRIWGhu\"\n",
      "CONSUMER_SECRET = \"OJVCVs1sG5RxOR1XRn30rj2x5BoPZvPzmSmA8kfLMBr2JjH5yZ\"\n",
      "\n",
      "ACCESS_TOKEN_KEY = \"105892440-hiutXI6zWd1XjrQJaotg7GbW6Mt1gihXCnE4njZH\"\n",
      "ACCESS_TOKEN_SECRET = \"RxIHlIylRycp8dPZfV8fXSM2WtMP74lteIp5P6jxwh4XW\"\n",
      "\n",
      "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
      "                  consumer_secret=CONSUMER_SECRET, \n",
      "                  access_token_key=ACCESS_TOKEN_KEY, \n",
      "                  access_token_secret=ACCESS_TOKEN_SECRET)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043c\u0435\u0442\u043e\u0434 GetUserTimeline \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 python-twitter. \u041e\u043d \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u0435 \u0431\u043e\u043b\u0435\u0435 200 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.\n",
      "\n",
      "\u041c\u0435\u0442\u043e\u0434 \u0438\u043c\u0435\u0435\u0442 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0443. \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0434\u043e\u0436\u0434\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u043e\u0432\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u043a API \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043c\u0435\u0442\u043e\u0434 `GetSleepTime`. \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e\u0431 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 `GetUserTimeLine` \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u044b\u0437\u044b\u0432\u0430\u0442\u044c `GetSleepTime` \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \"statuses/user_timeline\".\n",
      "\n",
      "\u041c\u0435\u0442\u043e\u0434 GetUserTimeline \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0442\u0438\u043f\u0430 Status. \u0423 \u044d\u0442\u0438\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0435\u0441\u0442\u044c \u043c\u0435\u0442\u043e\u0434 AsDict, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0442\u0432\u0438\u0442 \u0432 \u0432\u0438\u0434\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n",
      "\n",
      "Id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0438\u0437 \u0444\u0430\u0439\u043b\u0430, \u043a\u0430\u043a \u0431\u044b\u043b\u043e \u0441\u0434\u0435\u043b\u0430\u043d\u043e \u0432 \u0414\u0417 1.\n",
      "\n",
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `get_user_tweets(user_id)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438\u0437 \u0444\u0430\u0439\u043b\u0430. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f, \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u0432\u0438\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d \u0432 \u0432\u0438\u0434\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_user_tweets(user_id):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_user_tweets(user_id):\n",
      "    try:\n",
      "        timeline = api.GetUserTimeline(user_id=user_id, trim_user=True, exclude_replies=True, count=200)\n",
      "    except twitter.TwitterError as ex:\n",
      "        if ex.message == u\"Not authorized.\":\n",
      "            return None\n",
      "        \n",
      "        if ex.message[0]['code'] == 88:\n",
      "            sleep_time = api.GetSleepTime(\"statuses/user_timeline\")\n",
      "            print \"sleep for \", sleep_time\n",
      "            time.sleep(sleep_time)\n",
      "            timeline = api.GetUserTimeline(user_id=user_id, trim_user=True, exclude_replies=True, count=200)\n",
      "            \n",
      "    tweets = []\n",
      "    for item in timeline:\n",
      "        if item.lang != 'en':\n",
      "            continue\n",
      "\n",
      "        if item.retweeted:\n",
      "            continue\n",
      "\n",
      "        if item.truncated:\n",
      "            continue\n",
      "\n",
      "        if item.user_mentions:\n",
      "            continue\n",
      "\n",
      "        if item.media:\n",
      "            continue\n",
      "\n",
      "        if item.urls:\n",
      "            continue\n",
      "        \n",
      "        if 'http' in item.text:\n",
      "            continue\n",
      "            \n",
      "        tweets.append(item.AsDict())\n",
      "        \n",
      "    if len(tweets) > 10:\n",
      "        return tweets\n",
      "    \n",
      "    return None\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u0420\u0430\u0437\u0431\u043e\u0440 \u0442\u0435\u043a\u0441\u0442\u0430 \u0442\u0432\u0438\u0442\u0430"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b - \u043f\u0430\u0440\u0430\u0433\u0440\u0430\u0444\u044b, \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0441\u043b\u043e\u0432\u0430. \u041c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0442\u0432\u0438\u0442\u0430 \u043a \u0441\u043b\u043e\u0432\u0430\u043c. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0442\u0435\u043a\u0441\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u043d\u0430 \u0441\u043b\u043e\u0432\u0430. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e\u0433\u043e \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f.\n",
      "\n",
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e, `get_words(text)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u0441\u0442\u0440\u043e\u043a\u0430 \u0441 \u0442\u0435\u043a\u0441\u0442\u043e\u043c. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0441\u0442\u0440\u043e\u043a (\u0441\u043b\u043e\u0432). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_words(text):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "pattern = \"(?x)([A-Z]\\.)+|\\$?\\d+(\\.\\d+)?%?|\\w+([-']\\w+)*|[+/\\-@&*]\"\n",
      "\n",
      "\n",
      "def get_words(text):\n",
      "    words = [t.lower() for t in nltk.regexp_tokenize(text, pattern)]\n",
      "    return words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0414\u0430\u043b\u0435\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435. \u0422\u043e \u0435\u0441\u0442\u044c \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0438\u0445 \u043a \u0444\u043e\u0440\u043c\u0435 \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438 \u043f\u0440. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk. \u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043f\u043e \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0435, \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0438 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\u0445 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 http://www.nltk.org/\n",
      "\n",
      "\u0414\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0432\u0441\u0435\u0445 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0441\u043b\u043e\u0432\u0430\u0440\u0435\u0439 \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c download \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0414\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441\u043b\u043e\u0432\u0430 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u044b \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443. \n",
      "\n",
      "\u0414\u043b\u044f \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c `WordNetLemmatizer` \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk. \u0423 \u044d\u0442\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0435\u0441\u0442\u044c \u043c\u0435\u0442\u043e\u0434 `lemmatize`.\n",
      "\n",
      "\u0422\u0430\u043a\u0436\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0443\u0431\u0440\u0430\u0442\u044c \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430 \u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430. \u042d\u0442\u043e \u0447\u0430\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u0441\u043b\u043e\u0432\u0430, \u043d\u0435 \u043d\u0435\u0441\u0443\u0449\u0438\u0435 \u0441\u043c\u044b\u0441\u043b\u043e\u0432\u043e\u0439 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0438\u0445 \u0437\u0430\u0434\u0430\u0447. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e `stopwords` \u0438\u0437 nltk.corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "word = 'WORDS'\n",
      "word_nf = wnl.lemmatize(word.lower())\n",
      "print word_nf\n",
      "if word_nf in stop:\n",
      "    print \"This is stopword\"\n",
      "else:\n",
      "    print \"This is not stopword\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "word\n",
        "This is not stopword\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `get_tokens(words)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0441\u043b\u043e\u0432. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0442\u043e\u043a\u0435\u043d\u043e\u0432."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tokens(words):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "def get_tokens(words):\n",
      "    tokens = [wnl.lemmatize(t.lower()) for t in words]\n",
      "    return [i for i in tokens if i not in stop]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `get_tweet_tokens(tweet)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u0442\u0435\u043a\u0441\u0442 \u0442\u0432\u0438\u0442\u0430. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 -- \u0442\u043e\u043a\u0435\u043d\u044b \u0442\u0432\u0438\u0442\u0430."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tweet_tokens(tweet):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "pattern = \"(?x)([A-Z]\\.)+|\\$?\\d+(\\.\\d+)?%?|\\w+([-']\\w+)*|[+/\\-@&*]\"\n",
      "email_re = re.compile(r'[\\w\\-][\\w\\-\\.]+@[\\w\\-][\\w\\-\\.]+[a-zA-Z]{1,4}')\n",
      "nick_re = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-\\.]))@([A-Za-z_]+[A-Za-z0-9_]+)')\n",
      "\n",
      "def get_tweet_tokens(tweet):\n",
      "    text = tweet['text']\n",
      "    for email in re.findall(email_re, text):\n",
      "        if email.startswith('//'):\n",
      "            continue\n",
      "        text = text.replace(email, 'email_token')\n",
      "        \n",
      "    for nick in re.findall(nick_re, text):\n",
      "        text = text.replace('@'+nick, 'nick_token')\n",
      "    words = get_words(text)\n",
      "    return get_tokens(words)\n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `collect_users_tokens()`. \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043e\u043b\u0436\u043d\u0430 \u0441\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u0412 \u044d\u0442\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u0435 \u0441\u0442\u0440\u043e\u043a\u0430 - \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c. \u0421\u0442\u043e\u043b\u0431\u0435\u0446 - \u0442\u043e\u043a\u0435\u043d. \u041d\u0430 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0438 - \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u0442\u043e\u043a\u0435\u043d \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.\n",
      "\u0414\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c `DictVectorizer` \u0438\u0437 `sklearn.feature_extraction`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def collect_users_tokens():\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from collections import defaultdict\n",
      "import json\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "def collect_users_tokens(): \n",
      "    TRAINING_SET_URL = \"http://anokhin.github.io/users.txt\"\n",
      "    df_users = pd.read_csv(TRAINING_SET_URL, sep=\"\\t\", header=None, names=[\"user_id\", \"class\"])\n",
      "    \n",
      "    user_tweets = open(\"user_tweets.txt\", \"w\")\n",
      "    \n",
      "    vectorizer = DictVectorizer()\n",
      "    \n",
      "    users = []\n",
      "    \n",
      "    for index, row in df_users.iterrows():\n",
      "        user_id = row['user_id']\n",
      "        tweets = get_user_tweets(user_id)\n",
      "        if not tweets:\n",
      "            continue\n",
      "            \n",
      "        user_tweets.write(json.dumps((user_id, tweets)))\n",
      "        user_tweets.write('\\n')\n",
      "        \n",
      "        \n",
      "        user_to_tokens = defaultdict(int)\n",
      "        \n",
      "        for tweet in tweets:\n",
      "            tokens = get_tweet_tokens(tweet)\n",
      "            print tokens\n",
      "            if not tokens:\n",
      "                continue\n",
      "            for token in tokens:\n",
      "                user_to_tokens[token] += 1\n",
      "        \n",
      "        users.append(user_to_tokens)\n",
      "        \n",
      "        if index > 1:\n",
      "            break\n",
      "        \n",
      "    user_tweets.close()\n",
      "    \n",
      "    vectorized_sparse = vectorizer.fit_transform(users)\n",
      "    print vectorized_sparse\n",
      "    return vectorizer, vectorized_sparse\n",
      "    \n",
      "v, vs = collect_users_tokens()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u\"i'm\", u'floor', u'tweeting', u'prob', u'w', u'black', u'eye', u'n', u'swollen', u'nose']\n",
        "[u'mind', u'u', u'really', u'bad', u'boo', u'boo', u'toe', u'kicked', u'wood', u'door', u'frame', u\"i'm\", u'naked', u'bc', u'hate', u'sleeping', u'w', u'clothes', u'&', u'amp', u'mom', u'woke']\n",
        "[u\"i'm\", u'comfy', u'right', u'&', u'amp', u'realize', u'pee', u'go', u'get', u'call', u'&', u'amp', u'run', u'go', u'get', u\"it's\", u'dark', u'&', u'amp', u'slam', u'nose', u'/', u'toe', u'wall']\n",
        "[u'cousin', u'came', u'hate', u'kid', u'turn', u'live', u\"i'm\", u'good', u'bc', u\"didn't\", u'go', u'near', u'proof', u'always', u'right']\n",
        "[u'easy', u'hate', u'take', u'strength', u'gentle', u'kind', u'smith']\n",
        "[u'420', u'monday', u'year', u'whyyyyy']\n",
        "[u'smoked', u'bathroom', u'lolll']\n",
        "[u'doctor', u'making', u'use', u'3', u'pillow', u'plastic', u'allergy', u'case', u\"i'm\", u'pissed']\n",
        "[u'skipped', u'school', u'yesterday', u'pretended', u'sick', u'today', u\"i'm\", u'actually', u'sick', u'poetic', u'justice', u'dramatic', u'irony']\n",
        "[u'lately', u'year', u'specially', u\"i've\", u'sick', u'like', u'every', u'single', u'day', u'think', u'one', u'month', u'year', u\"wasn't\", u'sick']\n",
        "[u\"i'm\", u'fuckin', u'tired']\n",
        "[u\"don't\", u'care', u'hardcore', u'u', u\"don't\", u'lie', u'boy', u'1', u'd', u'zayn', u'hot', u'hell', u'haha']\n",
        "[u'zaynmalik', u'decides', u'solo', u'shit', u\"i'll\", u'ded', u'give', u'listen', u'bc', u'bet', u'boy', u'would', u'excellent', u'real', u'band']\n",
        "[u\"i'm\", u'even', u'fandom', u'anymore', u'make', u'sad', u'time', u'always', u'thought', u'would', u'better', u'outside', u'boyband']\n",
        "[u'anyone', u'1', u'd', u'fandom', u'dude', u'feel', u'u', u'guy', u'hard', u'angelo', u'left', u'miw', u'wa', u'upset', u'yall', u'get', u'thru', u'wa', u'fan', u'2', u'yr']\n",
        "[u'itdoesntevenmatteranywayeverytgingistemporarynothingifeelmattersandnoonecanhurtmemorethanicanhurtmyselfpainandlovemeannothingbcnothinglasts']\n",
        "[u'11', u'year', u'old', u'dying', u'rn', u'zaynmalik']\n",
        "[u'friend', u'car']\n",
        "[u'skipped', u'school', u'bc', u'y']\n",
        "[u'inherited', u'1970', u'yaht', u'fuck', u'yes', u\"it's\", u'gonna', u'groovy', u\"i'm\", u'gonna', u'live']\n",
        "[u\"i'm\", u'gonna', u'draw', u'shit', u'patch']\n",
        "[u'brought', u'drug', u'dog', u'school', u'today']\n",
        "[u'rlly', u'wanna', u'go', u'code', u'orange', u'show', u'tonight', u'ughhh']\n",
        "[u'gimme', u'pain', u'mcrbreakup']\n",
        "[u'fast', u'furious', u'7', u'emotional', u'ending']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'need', u'freakin', u'back', u'massage']\n",
        "[u'cold', u'suck']\n",
        "[u'birthday', u'4', u'day']\n",
        "[u'yo', u'come']\n",
        "[u'worst', u'day', u'ever', u'hand']\n",
        "[u'np', u'-', u'bedrock']\n",
        "[u'4.4', u'15', u'already']\n",
        "[u'bruuuuh', u\"can't\", u'sleep']\n",
        "[u'd.g.i.f.', u'u']\n",
        "[u'burping', u'sound', u'kinda', u'gross']\n",
        "[u'fav', u'4', u'dm', u\"i'm\", u'rly', u'bored']\n",
        "[u'suck', u'saving', u'money', u'/']\n",
        "[u'sleep', u'day', u'&', u'lt', u'&', u'lt', u'&', u'lt', u'&', u'gt', u'&', u'gt', u'&', u'gt']\n",
        "[u'lol', u'@', u'last', u'night']\n",
        "[u'bitch', u'better', u'moneeey']\n",
        "[u'long', u'night']\n",
        "[u'hit', u'u', u'upppppp']\n",
        "[u\"can't\", u'fall', u'asleep']\n",
        "[u'happy']\n",
        "[u'someone', u'go', u'vegan', u'22', u'day']\n",
        "[u'earliest', u\"i've\", u'bed', u'/']\n",
        "[u'feel', u'good', u'outside', u'-']\n",
        "[u'someone', u'come', u'dog', u'park', u'w', u'/']\n",
        "[u'smh', u'@', u'boy']\n",
        "[u'sexiest', u'thing', u'guy', u'hand']\n",
        "[u'fog', u'scary', u'af']\n",
        "[u'pcb']\n",
        "[u'ugh', u\"can't\", u'survive', u'w', u'/', u'o', u'oomf']\n",
        "[u'come']\n",
        "[u'come', u'dog', u'park']\n",
        "[u'bruh']\n",
        "[u'lame', u'night']\n",
        "[u'hit', u'lex', u'plz']\n",
        "[u\"there's\", u'thing', u'life', u\"that's\", u'better']\n",
        "[u'/', u'o', u'lh', u'boy', u'tho']\n",
        "[u'anything', u'going', u'tonight']\n",
        "[u'water', u'limestone', u'refreshing', u'randomthought']\n",
        "[u\"can't\", u'stop', u'watching', u'hart', u'dixie']\n",
        "[u'blessed']\n",
        "[u'snow']\n",
        "[u'jogger', u'life']\n",
        "[u'must', u'color', u'blind']\n",
        "[u'bored']\n",
        "[u'reese', u'witherspoon', u'slays', u'oscar']\n",
        "[u'earned', u'-', u'weeknd']\n",
        "[u'want', u'union', u'want', u'd', u'wadeee']\n",
        "[u'christian', u'grey', u'bae']\n",
        "[u'round', u'2', u'fifty', u'shade', u'tonight']\n",
        "[u'fifty', u'shade', u'grey', u'playlist']\n",
        "[u'fifty', u'shade', u'grey', u'changed', u'life']\n",
        "[u\"who's\"]\n",
        "[u'cant', u'believe', u'bruce', u'jenner', u'turning', u'woman']\n",
        "[u'need', u'plan']\n",
        "[u'plan', u'sunday']\n",
        "[u'swim', u'suit', u'shopping', u'best', u'kind', u'shopping']\n",
        "[u'love', u'weather', u'bad', u'snow', u'soon']\n",
        "[u'take', u'forever', u'understand', u'new', u'snapchat', u'update', u'/', u'/']\n",
        "[u'day', u'worry', u'want', u'eat']\n",
        "  (0, 122)\t1.0\n",
        "  (0, 62)\t1.0\n",
        "  (0, 309)\t1.0\n",
        "  (0, 79)\t2.0\n",
        "  (0, 17)\t1.0\n",
        "  (0, 177)\t1.0\n",
        "  (0, 117)\t4.0\n",
        "  (0, 133)\t1.0\n",
        "  (0, 305)\t1.0\n",
        "  (0, 132)\t3.0\n",
        "  (0, 145)\t1.0\n",
        "  (0, 229)\t1.0\n",
        "  (0, 238)\t1.0\n",
        "  (0, 135)\t1.0\n",
        "  (0, 15)\t1.0\n",
        "  (0, 37)\t1.0\n",
        "  (0, 102)\t1.0\n",
        "  (0, 297)\t1.0\n",
        "  (0, 40)\t1.0\n",
        "  (0, 88)\t1.0\n",
        "  (0, 139)\t1.0\n",
        "  (0, 191)\t1.0\n",
        "  (0, 310)\t2.0\n",
        "  (0, 210)\t1.0\n",
        "  (0, 4)\t1.0\n",
        "  :\t:\n",
        "  (1, 1)\t3.0\n",
        "  (1, 241)\t2.0\n",
        "  (1, 251)\t1.0\n",
        "  (1, 255)\t1.0\n",
        "  (1, 186)\t1.0\n",
        "  (1, 213)\t1.0\n",
        "  (1, 292)\t1.0\n",
        "  (1, 48)\t1.0\n",
        "  (1, 247)\t1.0\n",
        "  (1, 276)\t1.0\n",
        "  (1, 127)\t2.0\n",
        "  (1, 38)\t1.0\n",
        "  (1, 201)\t2.0\n",
        "  (1, 149)\t1.0\n",
        "  (1, 69)\t1.0\n",
        "  (1, 249)\t2.0\n",
        "  (1, 106)\t1.0\n",
        "  (1, 214)\t1.0\n",
        "  (1, 78)\t2.0\n",
        "  (1, 260)\t1.0\n",
        "  (1, 271)\t2.0\n",
        "  (1, 169)\t1.0\n",
        "  (1, 226)\t3.0\n",
        "  (1, 112)\t1.0\n",
        "  (1, 256)\t1.0\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "words = np.array(v.get_feature_names()) \n",
      "counts = vs.sum(axis=0).tolist()[0]\n",
      "freqs =  zip(words, counts)\n",
      "from pytagcloud import create_tag_image, make_tags\n",
      "tags = make_tags(freqs, maxsize=80)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create_tag_image(tags, \"tmp_file.png\", size=(1000, 480), fontname='Lobster')\n",
      "from PIL import Image                                                                       \n",
      "img = Image.open('tmp_file.png')\n",
      "img.show() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}