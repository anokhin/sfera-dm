{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "import mpl_toolkits.basemap as bm\n",
      "import twitter\n",
      "import requests\n",
      "import datetime\n",
      "import dateutil\n",
      "import csv\n",
      "\n",
      "# Plotting config\n",
      "%pylab inline\n",
      "import json\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['f']\n",
        "`%pylab --no-import-all` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TRAINING_SET_URL = \"http://anokhin.github.io/users.txt\"\n",
      "df_users = pd.read_csv(TRAINING_SET_URL, sep=\"\\t\", header=None, names=[\"user_id\", \"class\"])\n",
      "df_users.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>user_id</th>\n",
        "      <th>class</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  984121344</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  601849857</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  351429761</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 2792643764</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  215056389</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 2 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "      user_id  class\n",
        "0   984121344      0\n",
        "1   601849857      0\n",
        "2   351429761      0\n",
        "3  2792643764      0\n",
        "4   215056389      0\n",
        "\n",
        "[5 rows x 2 columns]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_ids = set(df_users['user_id'].values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_tweets = dict()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "with open('filtered_obama_followers_new.json_lines') as f:\n",
      "    for line in f:\n",
      "        u_id, tweets = json.loads(line)\n",
      "        if u_id not in user_ids:\n",
      "            continue\n",
      "        user_tweets[u_id] = set((tweet['text'] for tweet in tweets))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(user_tweets.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5000\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "with open('filtered_game_users_new.json_lines') as f:\n",
      "    for line in f:\n",
      "        u_id, tweets = json.loads(line)\n",
      "        if u_id not in user_ids:\n",
      "            continue\n",
      "        user_tweets[u_id] = set((tweet['text'] for tweet in tweets))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(user_tweets.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "with open('user_tweets_pickle', 'w') as f:\n",
      "    cPickle.dump(user_tweets, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "pattern = \"(?x)([A-Z]\\.)+|\\$?\\d+(\\.\\d+)?%?|\\w+([-']\\w+)*|[+/\\-@&*]\"\n",
      "\n",
      "\n",
      "def get_words(text):\n",
      "    words = [t.lower() for t in nltk.regexp_tokenize(text, pattern)]\n",
      "    return words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "def get_tokens(words):\n",
      "    tokens = [wnl.lemmatize(t.lower()) for t in words]\n",
      "    return [i for i in tokens if i not in stop]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "pattern = \"(?x)([A-Z]\\.)+|\\$?\\d+(\\.\\d+)?%?|\\w+([-']\\w+)*|[+/\\-@&*]\"\n",
      "email_re = re.compile(r'[\\w\\-][\\w\\-\\.]+@[\\w\\-][\\w\\-\\.]+[a-zA-Z]{1,4}')\n",
      "nick_re = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-\\.]))@([A-Za-z_]+[A-Za-z0-9_]+)')\n",
      "\n",
      "def get_tweet_tokens(text):\n",
      "    for email in re.findall(email_re, text):\n",
      "        if email.startswith('//'):\n",
      "            continue\n",
      "        text = text.replace(email, 'email_token')\n",
      "        \n",
      "    for nick in re.findall(nick_re, text):\n",
      "        text = text.replace('@'+nick, 'nick_token')\n",
      "    words = get_words(text)\n",
      "    return get_tokens(words)\n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_tokens = dict()\n",
      "for u_id, tweets in user_tweets.iteritems():\n",
      "    tokens = []\n",
      "    for tweet in tweets:\n",
      "        tokens += get_tweet_tokens(tweet)\n",
      "    user_tokens[u_id] = tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, v in user_tokens.iteritems():\n",
      "    print k, v\n",
      "    break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "984121344 [u'obama', u'slaying', u'rn', u'love', u'u', u'much', u'please', u'stop', u'making', u'feel', u'bad', u'trying', u'responsible', u'money', u'make', u'feel', u'useless', u'got', u'min', u'condition', u'sgt', u'pepper', u'lonely', u'heart', u'club', u'band', u'beales', u'cd', u'1987', u'2', u'buck', u'yassringoyass', u'groundhog', u'day', u'best', u'holliday', u\"it's\", u'want', u\"it's\", u'want', u\"why's\", u'much', u'pain', u\"it's\", u'2015', u\"it's\", u'time', u'yasssss', u'stateoftheunion', u'free', u'community', u'college', u'fuck', u'yes', u'college', u'stateoftheunion', u'obama', u'tip', u'toein', u\"jordan's\", u'rn', u'ferguson', u'stateoftheunion', u'mention', u'also', u'full', u'backpack', u'stuff', u'wanted', u'nap', u'ciarra', u'm', u'junsch', u'age', u'16', u'cause', u'death', u'goth', u'boy', u'scranton', u'decided', u'start', u'band', u'ruin', u'life', u'whatever', u'size', u'u', u'u', u'rock', u'stoop', u'calling', u'size', u'4', u'girl', u'thick', u'like', u'forreal', u\"that's\", u'literally', u'small', u'size', u'stop', u'thick', u'like', u'8', u'+', u'rant', u'wait', u'football', u'season', u'started', u'already', u\"it's\", u'super', u'bowl', u'wtf', u'disney', u'world', u'2', u'day', u'omfg', u'turning', u'rex', u'church', u\"it's\", u'ruined', u'ugh', u'stopchristians2k15', u'tf', u'doe', u'church', u'need', u'movie', u'theater', u'wtf', u'w', u'twitter', u'delete', u'profile', u'pic', u'tf', u'say', u\"i'm\", u'fine', u'mean', u'tom', u'left', u'blink-182', u'life', u'ha', u'lost', u'purpose', u'available', u'amazon', u'well', u'comb', u'knife', u'cat', u'key', u'knuckle', u'ring', u\"i'm\", u'done', u'apple', u\"i'm\", u'never', u'buying', u'iphone', u'bullshit']\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('user_tokens_pickle', 'w') as f:\n",
      "    cPickle.dump(user_tokens, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import DictVectorizer\n",
      "vectorizer = DictVectorizer()\n",
      "users = []\n",
      "token_counts = []\n",
      "for u_id, tokens in user_tokens.iteritems():\n",
      "    token_count = defaultdict(int)\n",
      "    for token in tokens:\n",
      "        token_count[token] += 1\n",
      "    users.append(u_id)\n",
      "    token_counts.append(token_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorized_sparse = vectorizer.fit_transform(token_counts)\n",
      "print vectorized_sparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 58789)\t1.0\n",
        "  (0, 140983)\t1.0\n",
        "  (0, 88686)\t1.0\n",
        "  (0, 150384)\t1.0\n",
        "  (0, 152614)\t1.0\n",
        "  (0, 48279)\t1.0\n",
        "  (0, 10572)\t1.0\n",
        "  (0, 35274)\t1.0\n",
        "  (0, 27703)\t1.0\n",
        "  (0, 8527)\t1.0\n",
        "  (0, 115217)\t1.0\n",
        "  (0, 22583)\t1.0\n",
        "  (0, 75354)\t1.0\n",
        "  (0, 3081)\t1.0\n",
        "  (0, 1884)\t1.0\n",
        "  (0, 4434)\t1.0\n",
        "  (0, 127235)\t1.0\n",
        "  (0, 80362)\t1.0\n",
        "  (0, 128187)\t2.0\n",
        "  (0, 14540)\t2.0\n",
        "  (0, 8836)\t1.0\n",
        "  (0, 29898)\t1.0\n",
        "  (0, 1771)\t2.0\n",
        "  (0, 152186)\t1.0\n",
        "  (0, 34737)\t2.0\n",
        "  :\t:\n",
        "  (9999, 11698)\t2.0\n",
        "  (9999, 9169)\t1.0\n",
        "  (9999, 112137)\t1.0\n",
        "  (9999, 64994)\t1.0\n",
        "  (9999, 137261)\t1.0\n",
        "  (9999, 117489)\t1.0\n",
        "  (9999, 20585)\t1.0\n",
        "  (9999, 97665)\t3.0\n",
        "  (9999, 63813)\t2.0\n",
        "  (9999, 136066)\t1.0\n",
        "  (9999, 84567)\t1.0\n",
        "  (9999, 72886)\t1.0\n",
        "  (9999, 547)\t1.0\n",
        "  (9999, 153319)\t1.0\n",
        "  (9999, 150892)\t2.0\n",
        "  (9999, 108387)\t1.0\n",
        "  (9999, 111111)\t1.0\n",
        "  (9999, 140673)\t2.0\n",
        "  (9999, 65631)\t1.0\n",
        "  (9999, 108851)\t1.0\n",
        "  (9999, 144359)\t1.0\n",
        "  (9999, 58552)\t5.0\n",
        "  (9999, 153325)\t1.0\n",
        "  (9999, 29562)\t1.0\n",
        "  (9999, 140062)\t1.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2507: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('users_and_tokens_freq_pickle', 'w') as f:\n",
      "    cPickle.dump((users, token_counts), f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}