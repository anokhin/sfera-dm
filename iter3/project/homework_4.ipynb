{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435 4. \u041a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438\u0437 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u0421\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u041f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f - \u0441\u0431\u043e\u0440 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0442\u044c\u0441\u044f \u043a Twitter API \u0438 \u0437\u0430\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0442\u044c \u0442\u0432\u0438\u0442\u044b \u043f\u043e id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f. \n",
      "\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a API \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u043e \u0432 \u0414\u0417 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "\n",
      "CONSUMER_KEY = \"NIdJmEzKanvYMoZMQOLRIWGhu\"\n",
      "CONSUMER_SECRET = \"OJVCVs1sG5RxOR1XRn30rj2x5BoPZvPzmSmA8kfLMBr2JjH5yZ\"\n",
      "\n",
      "ACCESS_TOKEN_KEY = \"105892440-hiutXI6zWd1XjrQJaotg7GbW6Mt1gihXCnE4njZH\"\n",
      "ACCESS_TOKEN_SECRET = \"RxIHlIylRycp8dPZfV8fXSM2WtMP74lteIp5P6jxwh4XW\"\n",
      "\n",
      "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
      "                  consumer_secret=CONSUMER_SECRET, \n",
      "                  access_token_key=ACCESS_TOKEN_KEY, \n",
      "                  access_token_secret=ACCESS_TOKEN_SECRET)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043c\u0435\u0442\u043e\u0434 GetUserTimeline \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 python-twitter. \u041e\u043d \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u0435 \u0431\u043e\u043b\u0435\u0435 200 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.\n",
      "    \u041c\u0435\u0442\u043e\u0434 \u0438\u043c\u0435\u0435\u0442 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0443. \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0434\u043e\u0436\u0434\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u043e\u0432\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u043a API \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043c\u0435\u0442\u043e\u0434 GetSleepTime. \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e\u0431 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 GetUserTimeLine \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u044b\u0437\u044b\u0432\u0430\u0442\u044c GetSleepTime \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \"statuses/user_timeline\".\n",
      "    \u041c\u0435\u0442\u043e\u0434 GetUserTimeline \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0442\u0438\u043f\u0430 Status. \u0423 \u044d\u0442\u0438\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0435\u0441\u0442\u044c \u043c\u0435\u0442\u043e\u0434 AsDict, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0442\u0432\u0438\u0442 \u0432 \u0432\u0438\u0434\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n",
      "    Id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0438\u0437 \u0444\u0430\u0439\u043b\u0430, \u043a\u0430\u043a \u0431\u044b\u043b\u043e \u0441\u0434\u0435\u043b\u0430\u043d\u043e \u0432 \u0414\u0417 1.\n",
      "    \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e get_user_tweets(user_id). \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438\u0437 \u0444\u0430\u0439\u043b\u0430. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f, \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u0432\u0438\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d \u0432 \u0432\u0438\u0434\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_user_tweets(user_id):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_user_tweets(user_id):\n",
      "    try:\n",
      "        timeline = api.GetUserTimeline(user_id=user_id, trim_user=True, exclude_replies=True, count=200)\n",
      "    except twitter.TwitterError as ex:\n",
      "        if ex.message == u\"Not authorized.\":\n",
      "            return None\n",
      "        \n",
      "        if ex.message[0]['code'] == 88:\n",
      "            sleep_time = api.GetSleepTime(\"statuses/user_timeline\")\n",
      "            print \"sleep for \", sleep_time\n",
      "            time.sleep(sleep_time)\n",
      "            timeline = api.GetUserTimeline(user_id=user_id, trim_user=True, exclude_replies=True, count=200)\n",
      "            \n",
      "    tweets = []\n",
      "    for item in timeline:\n",
      "        if item.lang != 'en':\n",
      "            continue\n",
      "\n",
      "        if item.retweeted:\n",
      "            continue\n",
      "\n",
      "        if item.truncated:\n",
      "            continue\n",
      "\n",
      "        if item.user_mentions:\n",
      "            continue\n",
      "\n",
      "        if item.media:\n",
      "            continue\n",
      "\n",
      "        if item.urls:\n",
      "            continue\n",
      "        \n",
      "        if 'http' in item.text:\n",
      "            continue\n",
      "            \n",
      "\n",
      "        tweets.append(item.AsDict())\n",
      "        \n",
      "    if len(tweets) > 10:\n",
      "        return tweets\n",
      "    \n",
      "    return None\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u0420\u0430\u0437\u0431\u043e\u0440 \u0442\u0435\u043a\u0441\u0442\u0430 \u0442\u0432\u0438\u0442\u0430"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b - \u043f\u0430\u0440\u0430\u0433\u0440\u0430\u0444\u044b, \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0441\u043b\u043e\u0432\u0430. \u041c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0442\u0432\u0438\u0442\u0430 \u043a \u0441\u043b\u043e\u0432\u0430\u043c. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0442\u0435\u043a\u0441\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u043d\u0430 \u0441\u043b\u043e\u0432\u0430. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e\u0433\u043e \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f.\n",
      "    \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e, get_words(text). \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u0441\u0442\u0440\u043e\u043a\u0430 \u0441 \u0442\u0435\u043a\u0441\u0442\u043e\u043c. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0441\u0442\u0440\u043e\u043a (\u0441\u043b\u043e\u0432). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_words(text):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "pattern = \"(?x)([A-Z]\\.)+|\\$?\\d+(\\.\\d+)?%?|\\w+([-']\\w+)*|[+/\\-@&*]\"\n",
      "\n",
      "\n",
      "def get_words(text):\n",
      "    words = [t.lower() for t in nltk.regexp_tokenize(text, pattern)]\n",
      "    return words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u0414\u0430\u043b\u0435\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435. \u0422\u043e \u0435\u0441\u0442\u044c \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0438\u0445 \u043a \u0444\u043e\u0440\u043c\u0435 \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438 \u043f\u0440. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk. \u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043f\u043e \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0435, \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0438 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\u0445 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 http://www.nltk.org/\n",
      "    \u0414\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0432\u0441\u0435\u0445 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0441\u043b\u043e\u0432\u0430\u0440\u0435\u0439 \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c download \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u0414\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441\u043b\u043e\u0432\u0430 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u044b \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443. \n",
      "    \u0414\u043b\u044f \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c WordNetLemmatizer \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk. \u0423 \u044d\u0442\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0435\u0441\u0442\u044c \u043c\u0435\u0442\u043e\u0434 lemmatize.\n",
      "    \u0422\u0430\u043a\u0436\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0443\u0431\u0440\u0430\u0442\u044c \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430 \u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430. \u042d\u0442\u043e \u0447\u0430\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u0441\u043b\u043e\u0432\u0430, \u043d\u0435 \u043d\u0435\u0441\u0443\u0449\u0438\u0435 \u0441\u043c\u044b\u0441\u043b\u043e\u0432\u043e\u0439 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0438\u0445 \u0437\u0430\u0434\u0430\u0447. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e stopwords \u0438\u0437 nltk.corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "word = 'WORDS'\n",
      "word_nf = wnl.lemmatize(word.lower())\n",
      "print word_nf\n",
      "if word_nf in stop:\n",
      "    print \"This is stopword\"\n",
      "else:\n",
      "    print \"This is not stopword\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "word\n",
        "This is not stopword\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e get_tokens(words). \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0441\u043b\u043e\u0432. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0442\u043e\u043a\u0435\u043d\u043e\u0432."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tokens(words):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "\n",
      "def get_tokens(words):\n",
      "    tokens = [wnl.lemmatize(t.lower()) for t in words]\n",
      "    return [i for i in tokens if i not in stop]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e get_tweet_tokens(tweet). \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u0442\u0435\u043a\u0441\u0442 \u0442\u0432\u0438\u0442\u0430. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u0442\u043e\u043a\u0435\u043d\u044b \u0442\u0432\u0438\u0442\u0430."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tweet_tokens(tweet):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "stop = stopwords.words('english')\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "\n",
      "pattern = \"(?x)([A-Z]\\.)+|\\$?\\d+(\\.\\d+)?%?|\\w+([-']\\w+)*|[+/\\-@&*]\"\n",
      "email_re = re.compile(r'[\\w\\-][\\w\\-\\.]+@[\\w\\-][\\w\\-\\.]+[a-zA-Z]{1,4}')\n",
      "nick_re = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-\\.]))@([A-Za-z_]+[A-Za-z0-9_]+)')\n",
      "\n",
      "def get_tweet_tokens(tweet):\n",
      "    text = tweet['text']\n",
      "    for email in re.findall(email_re, text):\n",
      "        if email.startswith('//'):\n",
      "            continue\n",
      "        text = text.replace(email, 'email_token')\n",
      "        \n",
      "    for nick in re.findall(nick_re, text):\n",
      "        text = text.replace('@'+nick, 'nick_token')\n",
      "    words = get_words(text)\n",
      "    return get_tokens(words)\n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "    \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e collect_users_tokens(). \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043e\u043b\u0436\u043d\u0430 \u0441\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u0412 \u044d\u0442\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u0435 \u0441\u0442\u0440\u043e\u043a\u0430 - \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c. \u0421\u0442\u043e\u043b\u0431\u0435\u0446 - \u0442\u043e\u043a\u0435\u043d. \u041d\u0430 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0438 - \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u0442\u043e\u043a\u0435\u043d \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.\n",
      "    \u0414\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c DictVectorizer \u0438\u0437 sklearn.feature_extraction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def collect_users_tokens():\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from collections import defaultdict\n",
      "import json\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "def collect_users_tokens(): \n",
      "    TRAINING_SET_URL = \"http://anokhin.github.io/users.txt\"\n",
      "    df_users = pd.read_csv(TRAINING_SET_URL, sep=\"\\t\", header=None, names=[\"user_id\", \"class\"])\n",
      "    \n",
      "    user_tweets = open(\"user_tweets.txt\", \"w\")\n",
      "    \n",
      "    vectorizer = DictVectorizer()\n",
      "    \n",
      "    users = []\n",
      "    \n",
      "    for index, row in df_users.iterrows():\n",
      "        user_id = row['user_id']\n",
      "        tweets = get_user_tweets(user_id)\n",
      "        if not tweets:\n",
      "            continue\n",
      "            \n",
      "        user_tweets.write(json.dumps((user_id, tweets)))\n",
      "        user_tweets.write('\\n')\n",
      "        \n",
      "        \n",
      "        user_to_tokens = defaultdict(int)\n",
      "        \n",
      "        for tweet in tweets:\n",
      "            tokens = get_tweet_tokens(tweet)\n",
      "            print tokens\n",
      "            if not tokens:\n",
      "                continue\n",
      "            for token in tokens:\n",
      "                user_to_tokens[token] += 1\n",
      "        \n",
      "        users.append(user_to_tokens)\n",
      "        \n",
      "        if index > 1:\n",
      "            break\n",
      "        \n",
      "    user_tweets.close()\n",
      "    \n",
      "    vectorized_sparse = vectorizer.fit_transform(users)\n",
      "    print vectorized_sparse\n",
      "    \n",
      "collect_users_tokens()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'i', u'will', u'mail', u'myself', u'to', u'them']\n",
        "None\n",
        "[u'motionless', u'in', u'white', u'turns', u'me', u'into', u'a', u'fuckin', u'beatles', u'era', u'fangirl', u'in', u'2', u'seconds', u'flat', u\"it's\", u'just', u'so', u'good']\n",
        "None\n",
        "[u'buy', u'me', u'jeffrey', u\"campbell's\", u'and', u\"i'll\", u'be', u'ur', u'slave', u'but', u'a', u'slave', u'with', u'perf', u'shoes']\n",
        "None\n",
        "[u'kiss', u'me', u'miss', u'me', u'diss', u'me', u'fist', u'me']\n",
        "None\n",
        "[u'love', u'me', u'hate', u'me', u'fuck', u'me', u'hug', u'me']\n",
        "None\n",
        "[u'sing', u'me', u'to', u'sleep', u\"i'm\", u'tired', u'and', u'i', u'i', u'want', u'to', u'got', u'to', u'bed', u\"don't\", u'try', u'to', u'wake', u'me', u'in', u'the', u'morning', u'cause', u'i', u'will', u'be', u'gone', u'restinpower']\n",
        "None\n",
        "[u'amazon', u'prime', u'took', u'off', u'99', u'percent', u'and', u'99', u'percenters', u'by', u'the', u'ministry', u'from', u'my', u'library', u'convinced', u'the', u'99%', u'are', u'responsible', u'for', u'this', u'travesty']\n",
        "None\n",
        "[u'somewhere', u'in', u'between', u'you', u'and', u'i', u'i', u'lost', u'me', u'and', u'myself']\n",
        "None\n",
        "[u'filled', u'out', u'an', u'application', u'for', u'milton', u'bakery', u'fingerscrossed', u'good', u'with', u'people', u'hahaha', u'lol', u'what', u'a', u'lie']\n",
        "None\n",
        "[u'my', u'mom', u'buys', u'200', u'vcr', u'to', u'record', u'shows', u'*', u'turn', u'tv', u'off', u'every', u'single', u'time', u'she', u'leaves', u'the', u'room', u'*', u'*', u'gets', u'mad', u'when', u'grays', u'anatomy', u'doesnt', u'record', u'*']\n",
        "None\n",
        "[u'did', u'i', u'mention', u'i', u'also', u'have', u'a', u'full', u'backpack', u'of', u'stuff']\n",
        "None\n",
        "[u'disney', u'world', u'in', u'2', u'days', u'omfg']\n",
        "None\n",
        "[u'just', u'got', u'a', u'min', u'condition', u'sgt', u'peppers', u'lonely', u'hearts', u'club', u'band', u'by', u'the', u'beales', u'cd', u'from', u'1987', u'for', u'2', u'bucks', u'yassringoyass']\n",
        "None\n",
        "[u'groundhog', u'day', u'is', u'the', u'best', u'holliday']\n",
        "None\n",
        "[u'wait', u'football', u'season', u'started', u'already', u\"it's\", u'the', u'super', u'bowl', u'wtf']\n",
        "None\n",
        "[u'rant', u'over']\n",
        "None\n",
        "[u'i', u'just', u'wanted', u'to', u'nap']\n",
        "None\n",
        "[u'i', u'love', u'u', u'so', u'much', u'but', u'please', u'stop', u'making', u'me', u'feel', u'bad', u'for', u'trying', u'to', u'be', u'responsible', u'with', u'my', u'money', u'it', u'makes', u'me', u'feel', u'so', u'useless']\n",
        "None\n",
        "[u'whatever', u'size', u'u', u'are', u'u', u'rock', u'it', u'but', u'stoop', u'calling', u'size', u'4', u'girls', u'thick', u'like', u'forreal', u\"that's\", u'literally', u'a', u'small', u'size', u'just', u'stop', u'thick', u'is', u'like', u'8', u'+']\n",
        "None\n",
        "[u'if', u\"it's\", u'what', u'he', u'wants', u'and', u\"it's\", u'what', u'she', u'wants', u'then', u\"why's\", u'there', u'so', u'much', u'pain']\n",
        "None\n",
        "[u'what', u'she', u'says', u\"i'm\", u'fine', u'what', u'she', u'means', u'tom', u'left', u'blink-182', u'and', u'my', u'life', u'has', u'lost', u'purpose']\n",
        "None\n",
        "[u'they', u'are', u'turning', u'the', u'rex', u'into', u'a', u'church', u'and', u\"it's\", u'ruined', u'now', u'ugh', u'stopchristians2k15', u'why', u'tf', u'does', u'a', u'church', u'need', u'a', u'movie', u'theater']\n",
        "None\n",
        "[u\"i'm\", u'so', u'done', u'with', u'apple', u\"i'm\", u'never', u'buying', u'an', u'iphone', u'again', u'this', u'is', u'bullshit']\n",
        "None\n",
        "[u'wtf', u'is', u'up', u'w', u'twitter', u'and', u'why', u'did', u'it', u'delete', u'my', u'profile', u'pic', u'tf']\n",
        "None\n",
        "[u'ciarra', u'm', u'junsch', u'age', u'16', u'cause', u'of', u'death', u'goth', u'boys', u'from', u'scranton', u'decided', u'to', u'start', u'a', u'band', u'and', u'ruin', u'my', u'life']\n",
        "None\n",
        "[u'so', u\"don't\", u'just', u'plan', u'go', u'for', u'it', u'because', u'you', u'only', u'live', u'once', u'you', u'will', u'never', u'have', u'so', u'much', u'time', u'again', u'travelling', u'living', u'letsgetlost']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "None\n",
        "[u'you', u'will', u'start', u'feeling', u'lot', u'more', u'independent', u'and', u'will', u'come', u'back', u'a', u'changed', u'person', u'travelling', u'living', u'letsgetlost']\n",
        "None\n",
        "[u'you', u'will', u'see', u'new', u'places', u'realize', u'new', u'things', u'and', u'will', u'also', u'make', u'friendships', u'that', u'will', u'last', u'a', u'lifetime', u'travelling', u'living', u'letsgetlost']\n",
        "None\n",
        "[u'travelling', u'is', u'a', u'beautiful', u'experience', u'and', u'you', u'learn', u'many', u'things', u'along', u'the', u'way', u'travelling', u'living', u'letsgetlost']\n",
        "None\n",
        "[u'letsgetlost', u'somewhere', u'where', u'no', u'one', u'knows', u'our', u'name']\n",
        "None\n",
        "[u'i', u'have', u'the', u'universe', u'inside']\n",
        "None\n",
        "[u'you', u'never', u'realize', u'how', u'lonely', u'you', u'are', u'untill', u\"it's\", u'the', u'end', u'of', u'the', u'day', u'and', u'you', u'got', u'a', u'bunch', u'of', u'stuff', u'to', u'talk', u'about', u'and', u'no', u'one', u'to', u'tell', u'it', u'to']\n",
        "None\n",
        "[u'there', u'are', u'so', u'many', u'people', u'i', u'want', u'to', u'punch', u'in', u'the', u'face']\n",
        "None\n",
        "[u'i', u'am', u'retweeting', u'so', u'many', u'pictures', u'of', u'poems', u'quotes', u'i', u'wonder', u'how', u'anyone', u'can', u'write', u'so', u'beautifull', u'&', u'amp', u'true', u'things', u'i', u'am', u'totally', u'into', u'it']\n",
        "None\n",
        "[u'my', u'instincts', u'are', u'supporting', u'pakistan', u'but', u'my', u'heart', u'wants', u'obvis', u'india', u'indvspak']\n",
        "None\n",
        "[u'hypocrisy', u'is', u'in', u'our', u'blood', u'hypocriteindia', u'hypocriteindians']\n",
        "None\n",
        "[u'everything', u'i', u'like', u'is', u'either', u'expensive', u'or', u'not', u'approved', u'by', u'our', u'indian', u'culture']\n",
        "None\n",
        "[u'reese', u'witherspoon', u'slays', u'the', u'oscars']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "None\n",
        "[u'earned', u'it', u'-', u'the', u'weeknd']\n",
        "None\n",
        "[u'they', u'want', u'that', u'union', u'they', u'want', u'that', u'd', u'wadeee']\n",
        "None\n",
        "[u'christian', u'grey', u'is', u'bae']\n",
        "None\n",
        "[u'round', u'2', u'of', u'fifty', u'shades', u'tonight']\n",
        "None\n",
        "[u'fifty', u'shades', u'of', u'grey', u'playlist', u'on']\n",
        "None\n",
        "[u'fifty', u'shades', u'of', u'grey', u'changed', u'my', u'life']\n",
        "None\n",
        "[u\"who's\", u'up']\n",
        "None\n",
        "[u'i', u'cant', u'believe', u'bruce', u'jenner', u'is', u'turning', u'into', u'a', u'woman']\n",
        "None\n",
        "[u'i', u'need', u'plans']\n",
        "None\n",
        "[u'any', u'plans', u'for', u'sunday']\n",
        "None\n",
        "[u'swim', u'suit', u'shopping', u'is', u'the', u'best', u'kind', u'of', u'shopping']\n",
        "None\n",
        "[u'i', u'love', u'this', u'weather', u'too', u'bad', u'it', u'will', u'snow', u'soon']\n",
        "None\n",
        "[u'i', u'take', u'forever', u'to', u'understand', u'the', u'new', u'snapchat', u'updates', u'/', u'/']\n",
        "None\n",
        "[u'those', u'days', u'where', u'all', u'you', u'have', u'to', u'worry', u'about', u'is', u'where', u'you', u'want', u'to', u'eat']\n",
        "None\n",
        "[u'i', u'need', u'more', u'tumblr', u'friends']\n",
        "None\n",
        "[u'salty', u'i', u\"didn't\", u'get', u'a', u'ticket', u'to', u'see', u'obama']\n",
        "None\n",
        "[u'thank', u'the', u'lord', u'for', u'iphone', u'6', u'plus', u'battery', u'life']\n",
        "None\n",
        "[u'this', u'weather', u'is', u'a', u'tease', u'for', u'spring']\n",
        "None\n",
        "[u'having', u'an', u'iphone', u'6', u'plus', u'fall', u'on', u'your', u'face', u'&', u'lt', u'&', u'lt', u'ouch']\n",
        "None\n",
        "[u'i', u'need', u'a', u'roommate']\n",
        "None\n",
        "[u'i', u'feel', u'like', u'pampering', u'myself', u'today']\n",
        "None\n",
        "[u'this', u'weekend', u'tho', u'-']\n",
        "None\n",
        "[u'wish', u'i', u'was', u'at', u'the', u'game', u'rn']\n",
        "None\n",
        "[u'when', u'i', u'get', u'bored', u'i', u'just', u're', u'watch', u'gossip', u'girl']\n",
        "None\n",
        "[u'ugh', u'craving', u'munchers']\n",
        "None\n",
        "[u'i', u'want', u'to', u'go', u'wine', u'tasting', u'latenightthoughts']\n",
        "None\n",
        "[u'john', u'legend', u'is', u'bae']\n",
        "None\n",
        "[u'fav', u'for', u'a', u'dm']\n",
        "None\n",
        "[u's', u'/', u'o', u'to', u'mom', u'for', u'taking', u'me', u'with', u'her', u'to', u'her', u'friends', u'birthday', u'party', u'where', u'there', u'happens', u'to', u'be', u'a', u'stripper']\n",
        "None\n",
        "[u'someone', u'get', u'some', u'food', u'with', u'me']\n",
        "None\n",
        "[u'bucket', u'list', u'go', u'to', u'paris', u'with', u'my', u'best', u'friends']\n",
        "None\n",
        "[u'where', u'can', u'i', u'order', u'some', u'ladur\\xe9e', u'macaroons', u'-', u'asap']\n",
        "None\n",
        "[u'if', u'you', u'look', u'through', u'my', u'favorites', u\"you'll\", u'probably', u'see', u'a', u'lot', u'of', u'food', u'porn', u'*', u'shrugs', u'*']\n",
        "None\n",
        "[u'when', u'you', u'go', u'from', u'seeing', u'someone', u'everyday', u'to', u'not', u'seeing', u'them', u'at', u'all', u'&', u'lt', u'&', u'lt', u'&', u'lt', u'&', u'lt']\n",
        "None\n",
        "[u'lex', u'lives', u'on', u'snapchat', u'lmao']\n",
        "None\n",
        "[u\"ain't\", u'no', u'party', u'like', u'a', u'new', u'york', u'party']\n",
        "None\n",
        "[u'any', u'parties', u'for', u'new', u'years']\n",
        "None\n",
        "[u'this', u'sucks']\n",
        "None\n",
        "[u'finally', u'got', u'my', u'license']\n",
        "None\n",
        "[u'play', u'me', u'on', u'trivia', u'crack', u'bored', u'justdoit']\n",
        "None\n",
        "[u'my', u'dog', u'chewed', u'the', u'power', u'cable', u'for', u'my', u'directv']\n",
        "None\n",
        "[u'can', u'it', u'please', u'snow', u'again']\n",
        "None\n",
        "[u'someone', u'come', u'over', u'and', u'bake', u'cookies', u'w', u'/', u'me', u'-']\n",
        "None\n",
        "[u'ariana', u'g', u'is', u'sold', u'out']\n",
        "None\n",
        "[u'i', u'need', u'to', u'see', u'j', u'cole', u'kendrick', u'b', u'and', u'j', u'weeknd', u'and', u'ariana', u'grande', u'live', u'goals']\n",
        "None\n",
        "[u'this', u'rain', u'is', u'clutch']\n",
        "None\n",
        "[u'can', u'i', u'go', u'home']\n",
        "None\n",
        "[u'i', u'hate', u'small', u'talk', u\"it's\", u'a', u'awkward']\n",
        "None\n",
        "[u'christmas', u'is', u'in', u'2', u'weeks']\n",
        "None\n",
        "[u'this', u'week', u\"couldn't\", u'be', u'any', u'slower']\n",
        "None\n",
        "[u'downtown', u'in', u'december']\n",
        "None\n",
        "[u'omg', u'can', u'someone', u'please', u'go', u'with', u'me', u'to', u'petland']\n",
        "None\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}